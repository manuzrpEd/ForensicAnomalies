{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8ddc94",
   "metadata": {},
   "source": [
    "# Anomaly Detection on LAPD Crime Data – Summary  \n",
    "*(Isolation Forest | contamination=5% | ~210,000 records)*\n",
    "\n",
    "## What the model actually learned\n",
    "\n",
    "### The “Typical” / Normal Crime in Los Angeles  \n",
    "(≈95% of all records – the green bars and “Normal Mode” columns)\n",
    "\n",
    "| Feature                    | Typical Value in Normal Records                                  | Evidence in your notebook |\n",
    "|----------------------------|------------------------------------------------------------------|---------------------------|\n",
    "| Number of victims          | **1**                                                            | `totalvictimcount` normal mode = 1 (comparison table & categorical summary) |\n",
    "| Number of offenses         | **1**                                                            | `totaloffensecount` normal mode = 1 |\n",
    "| Crime against              | **Property** (theft, burglary, vandalism, auto theft)           | `crime_against` normal mode = Property (comparison table) |\n",
    "| Victim sex                 | **Male**                                                         | `vict_sex` normal mode = M |\n",
    "| Victim age group           | **30–45**                                                        | `vict_age` normal mode = 30-45 |\n",
    "| Victim descent             | **Hispanic**                                                     | `vict_descent` normal mode = Hispanic |\n",
    "| Case status                | **Investigation Continued** (still open)                         | `status_desc` normal mode = Investigation Continued |\n",
    "| Weapon                     | **Missing / none recorded**                                      | `weapon_desc` normal mode = Missing |\n",
    "| Area                       | Olympic Division slightly most common                            | `area_name` normal mode = Olympic |\n",
    "| Homeless involvement       | **Almost never** (suspect, victim, or arrestee)                  | Green bars near 0 in “Numeric Features” chart |\n",
    "| Transit-related (bus/metro)| **Almost never**                                                 | Green bar at ≈0 in numeric chart |\n",
    "| Domestic violence flag     | **Almost never**                                                 | Green bar at ≈0 |\n",
    "| Gang-related flag          | **Almost never**                                                 | Green bar at ≈0 |\n",
    "| Weekend                    | Slightly more weekdays                                           | `is_weekend` normal mean ≈0.30 |\n",
    "\n",
    "→ The model perfectly captured the dominant reality of LAPD data: **routine, single-victim property crimes against Hispanic adult males that stay under investigation for months or longer**.\n",
    "\n",
    "### The Anomalous Crimes (the ~5% flagged as outliers)\n",
    "\n",
    "These are the records that break the above pattern in multiple ways at once.\n",
    "\n",
    "| Rank | Feature that makes it anomalous                                | Strength (from your outputs)                                 |\n",
    "|------|----------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| 1    | `homeless_arrestee_crime` = 1                                  | #1 in permutation importance bar chart                       |\n",
    "| 2    | `transit_related_crime` = 1                                    | #2 in permutation importance                                 |\n",
    "| 3    | `totalvictimcount` ≥ 2 (especially 4+)                         | Highest categorical distance (0.407) in comparison table    |\n",
    "| 4    | `status_desc` = “Cleared by Arrest”                            | 2nd highest categorical distance (0.406 )                     |\n",
    "| 5    | `crime_against` = “Person” (when combined with other flags)    | 3rd highest distance (0.350)                                 |\n",
    "| 6–10 | Multiple victims, homeless suspect/victim, White or Missing ethnicity, Business victim, Bodily force weapon, etc. | All appear in Top 10 of permutation importance + comparison table |\n",
    "\n",
    "### Real examples from “Detailed Anomaly Analysis” section  \n",
    "The top 5 flagged anomalies are classic examples of:\n",
    "\n",
    "- Mass-victim incidents on public transit with homeless arrestees  \n",
    "- Quickly solved violent assaults (bodily force) on the Metro  \n",
    "- Business victims robbed on buses/trains by homeless suspects  \n",
    "→ Exactly the crimes that dominate news headlines and council meetings, despite being statistically rare.\n",
    "\n",
    "### Bottom-line takeaway\n",
    "\n",
    "> The Isolation Forest did not just find statistical outliers — it automatically surfaced the tiny fraction of crimes that are **violent, homelessness-involved, transit-related, multi-victim, or rapidly solved** — precisely the incidents that generate the most public and political concern in Los Angeles, even though they represent only ~5% of reported crime.\n",
    "\n",
    "The anomaly detector is effectively functioning as a **high-impact / high-visibility crime early-warning system**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97cc8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "from anomaly_detection import (\n",
    "    visualize_anomaly_characteristics,\n",
    "    prepare_data_for_model,\n",
    "    fit_isolation_forest,\n",
    "    add_anomaly_labels,\n",
    "    get_anomaly_statistics,\n",
    "    print_anomaly_statistics,\n",
    "    visualize_anomaly_distribution,\n",
    "    get_feature_importance_for_anomalies,\n",
    "    visualize_feature_importance,\n",
    "    analyze_anomaly_characteristics,\n",
    "    show_detailed_anomaly_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309cee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 218472 records with 23 features\n",
      "Shape: (218472, 23)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load features\n",
    "df_features = pd.read_pickle(\"lapd_offenses_victims_features.pkl\")\n",
    "print(f\"Loaded {len(df_features)} records with {len(df_features.columns)} features\")\n",
    "print(f\"Shape: {df_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4452c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data encoded. Categorical columns encoded: 14\n",
      "area_name                  int64\n",
      "totaloffensecount          int64\n",
      "group                      int64\n",
      "nibr_description           int64\n",
      "crime_against              int64\n",
      "premis_desc                int64\n",
      "status_desc                int64\n",
      "totalvictimcount           int64\n",
      "victim_shot                int64\n",
      "domestic_violence_crime    int64\n",
      "hate_crime                 int64\n",
      "gang_related_crime         int64\n",
      "transit_related_crime      int64\n",
      "homeless_victim_crime      int64\n",
      "homeless_suspect_crime     int64\n",
      "homeless_arrestee_crime    int64\n",
      "weapon_desc                int64\n",
      "vict_age                   int64\n",
      "vict_descent               int64\n",
      "vict_sex                   int64\n",
      "victim_type                int64\n",
      "month                      int64\n",
      "is_weekend                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Prepare data\n",
    "df_encoded, label_encoders = prepare_data_for_model(df_features)\n",
    "print(f\"Data encoded. Categorical columns encoded: {len(label_encoders)}\")\n",
    "print(df_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "168ac65a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 4: Fit Isolation Forest\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model, predictions, anomaly_scores = \u001b[43mfit_isolation_forest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontamination\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Expect 5% anomalies\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIsolation Forest model fitted successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\anomaly_detection.py:65\u001b[39m, in \u001b[36mfit_isolation_forest\u001b[39m\u001b[34m(df, contamination, random_state)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03mFit Isolation Forest model for anomaly detection.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m \u001b[33;03m    Fitted model and anomaly predictions (-1 for anomalies, 1 for normal)\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m model = IsolationForest(\n\u001b[32m     59\u001b[39m     contamination=contamination, \u001b[38;5;66;03m# This is the expected proportion of outliers in the dataset.\u001b[39;00m\n\u001b[32m     60\u001b[39m     random_state=random_state,\n\u001b[32m     61\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,\n\u001b[32m     62\u001b[39m     max_samples=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     63\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m anomaly_scores = model.score_samples(df)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, predictions, anomaly_scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\base.py:1122\u001b[39m, in \u001b[36mOutlierMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m         warnings.warn(\n\u001b[32m   1109\u001b[39m             (\n\u001b[32m   1110\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `predict` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1119\u001b[39m         )\n\u001b[32m   1121\u001b[39m \u001b[38;5;66;03m# override for transductive outlier detectors like LocalOulierFactor\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.predict(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:381\u001b[39m, in \u001b[36mIsolationForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[32m    380\u001b[39m     X = X.tocsr()\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28mself\u001b[39m.offset_ = np.percentile(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m100.0\u001b[39m * \u001b[38;5;28mself\u001b[39m.contamination)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:540\u001b[39m, in \u001b[36mIsolationForest._score_samples\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    537\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Take the opposite of the scores as bigger is better (here less abnormal)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_chunked_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:570\u001b[39m, in \u001b[36mIsolationForest._compute_chunked_score_samples\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    566\u001b[39m scores = np.zeros(n_samples, order=\u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[32m    569\u001b[39m     \u001b[38;5;66;03m# compute score on the slices of test samples:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     scores[sl] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:606\u001b[39m, in \u001b[36mIsolationForest._compute_score_samples\u001b[39m\u001b[34m(self, X, subsample_features)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Note: we use default n_jobs value, i.e. sequential computation, which\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# we expect to be more performant that parallelizing for small number\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# of samples, e.g. < 1k samples. Default n_jobs value can be overridden\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# https://github.com/scikit-learn/scikit-learn/pull/28622 for more\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[38;5;66;03m# details.\u001b[39;00m\n\u001b[32m    605\u001b[39m lock = threading.Lock()\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msharedmem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_compute_tree_depths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decision_path_lengths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_average_path_length_per_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_features_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m denominator = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_) * average_path_length_max_samples\n\u001b[32m    625\u001b[39m scores = \u001b[32m2\u001b[39m ** (\n\u001b[32m    626\u001b[39m     \u001b[38;5;66;03m# For a single training sample, denominator and depth are 0.\u001b[39;00m\n\u001b[32m    627\u001b[39m     \u001b[38;5;66;03m# Therefore, we set the score manually to 1.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    630\u001b[39m     )\n\u001b[32m    631\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuz\\Desktop\\Adm\\Projects\\ForensicAnomalies\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:46\u001b[39m, in \u001b[36m_parallel_compute_tree_depths\u001b[39m\u001b[34m(tree, X, features, tree_decision_path_lengths, tree_avg_path_lengths, depths, lock)\u001b[39m\n\u001b[32m     42\u001b[39m     X_subset = X[:, features]\n\u001b[32m     44\u001b[39m leaves_index = tree.apply(X_subset, check_input=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtree_decision_path_lengths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleaves_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_avg_path_lengths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleaves_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 4: Fit Isolation Forest\n",
    "model, predictions, anomaly_scores = fit_isolation_forest(\n",
    "    df_encoded,\n",
    "    contamination=0.05,  # Expect 5% anomalies\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Isolation Forest model fitted successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Add anomaly labels\n",
    "df_results = add_anomaly_labels(df_features, predictions, anomaly_scores)\n",
    "print(df_results[['is_anomaly', 'anomaly_score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24023a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Statistics\n",
    "stats = get_anomaly_statistics(df_results)\n",
    "print_anomaly_statistics(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ce4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualize results\n",
    "visualize_anomaly_distribution(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance: change in the mean anomaly score\n",
    "feature_importance = get_feature_importance_for_anomalies(df_encoded, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd32db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "visualize_feature_importance(feature_importance, df_features, top_n=30)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze anomaly characteristics across features\n",
    "comparison = analyze_anomaly_characteristics(df_results, df_features)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize characteristics\n",
    "visualize_anomaly_characteristics(comparison, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed analysis\n",
    "show_detailed_anomaly_analysis(df_results, df_features, comparison, n_anomalies=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
